<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Theme Suggester ‚Äì ONNX WebGPU Demo</title>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/onnxruntime-web/1.17.1/ort.webgpu.min.js"></script>
  <style>
    * { margin:0; padding:0; box-sizing:border-box; }
    body {
      font-family:'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      min-height:100vh; display:flex; align-items:center; justify-content:center; padding:20px;
    }
    .container {
      background:white; border-radius:16px; box-shadow:0 20px 60px rgba(0,0,0,0.3);
      max-width:800px; width:100%; padding:40px;
    }
    h1 { color:#333; margin-bottom:10px; font-size:2em; }
    .subtitle { color:#666; margin-bottom:30px; font-size:0.9em; }
    .status { padding:15px; border-radius:8px; margin-bottom:20px;
      font-weight:500; display:flex; align-items:center; gap:10px;
    }
    .status.info { background:#e3f2fd; color:#1976d2; border-left:4px solid #1976d2; }
    .status.success { background:#e8f5e9; color:#388e3c; border-left:4px solid #388e3c; }
    .status.error { background:#ffebee; color:#c62828; border-left:4px solid #c62828; }
    .status.warning { background:#fff3e0; color:#e65100; border-left:4px solid #e65100; }
    .spinner {
      border:3px solid #f3f3f3; border-top:3px solid #667eea;
      border-radius:50%; width:20px; height:20px; animation:spin 1s linear infinite;
    }
    @keyframes spin { 0%{transform:rotate(0deg);}100%{transform:rotate(360deg);} }
    .input-group { margin-bottom:20px; }
    label { display:block; color:#555; font-weight:600; margin-bottom:8px; }
    textarea {
      width:100%; padding:12px; border:2px solid #ddd; border-radius:8px;
      font-size:14px; font-family:inherit; resize:vertical; min-height:100px;
      transition:border-color 0.3s;
    }
    textarea:focus { outline:none; border-color:#667eea; }
    button {
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      color:white; border:none; padding:14px 32px; border-radius:8px;
      font-size:16px; font-weight:600; cursor:pointer; transition:transform .2s,box-shadow .2s;
      width:100%;
    }
    button:hover:not(:disabled) { transform:translateY(-2px); box-shadow:0 8px 20px rgba(102,126,234,0.4); }
    button:disabled { opacity:0.6; cursor:not-allowed; }
    .output { margin-top:20px; padding:20px; background:#f8f9fa; border-radius:8px; border:2px solid #e9ecef; display:none; }
    .output.show { display:block; }
    .output h3 { color:#333; margin-bottom:12px; }
    .output-content { color:#555; line-height:1.6; white-space:pre-wrap; word-wrap:break-word; }
    .model-info { background:#f8f9fa; padding:15px; border-radius:8px; margin-bottom:20px; font-size:0.85em; color:#666; }
    .model-info strong { color:#333; }
  </style>
</head>
<body>
  <div class="container">
    <h1>üé® Theme Suggester</h1>
    <p class="subtitle">ONNX INT8 Model with WebGPU Acceleration</p>
    <div id="status" class="status info">
      <div class="spinner"></div><span>Checking WebGPU support...</span>
    </div>
    <div class="model-info">
      <strong>Model:</strong> anand-ai/theme-suggester-onnx-int8<br>
      <strong>Base:</strong> Qwen2.5-0.5B-Instruct (Fine-tuned on 20k samples)<br>
      <strong>Format:</strong> ONNX INT8 Quantized
    </div>
    <div class="input-group">
      <label for="input-text">Enter your prompt:</label>
      <textarea id="input-text" placeholder="Example: I want a theme for a tech startup website..."></textarea>
    </div>
    <button id="run-btn" disabled>Run Inference</button>
    <div id="output" class="output">
      <h3>Output:</h3>
      <div class="output-content" id="output-content"></div>
    </div>
  </div>

  <script type="module">
    import { Tokenizer } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@3.3.0/dist/tokenizer.esm.js';

    const modelUrl = 'https://huggingface.co/anand-ai/theme-suggester-onnx-int8/resolve/main/model_quantized.onnx';
    const tokenizerUrl = 'https://huggingface.co/anand-ai/theme-suggester-onnx-int8/resolve/main/tokenizer.json';

    const statusEl = document.getElementById('status');
    const runBtn = document.getElementById('run-btn');
    const inputText = document.getElementById('input-text');
    const outputEl = document.getElementById('output');
    const outputContent = document.getElementById('output-content');

    let session = null;
    let tokenizer = null;

    function updateStatus(message, type='info', showSpinner=false) {
      statusEl.className = `status ${type}`;
      statusEl.innerHTML = showSpinner 
        ? `<div class="spinner"></div><span>${message}</span>`
        : `<span>${message}</span>`;
    }

    async function checkWebGPU() {
      if (!navigator.gpu) {
        updateStatus('‚ùå WebGPU is not supported in this browser. Please use Chrome 113+ or Edge 113+', 'error');
        return false;
      }
      try {
        const adapter = await navigator.gpu.requestAdapter();
        if (!adapter) {
          updateStatus('‚ùå No WebGPU adapter found', 'error');
          return false;
        }
        updateStatus('‚úì WebGPU is supported and available', 'success');
        return true;
      } catch (e) {
        updateStatus(`‚ùå WebGPU error: ${e.message}`, 'error');
        return false;
      }
    }

    async function loadModel() {
      try {
        updateStatus('Loading model from Hugging Face...', 'info', true);
        // Configure ONNX Runtime for WebGPU
        ort.env.wasm.numThreads = 1;
        ort.env.wasm.simd = true;

        session = await ort.InferenceSession.create(modelUrl, {
          executionProviders: ['webgpu'],
          graphOptimizationLevel: 'all'
        });

        updateStatus('‚úì Model loaded successfully!', 'success');
        runBtn.disabled = false;

        console.log('Input names:', session.inputNames);
        console.log('Output names:', session.outputNames);
      } catch (error) {
        updateStatus(`‚ùå Error loading model: ${error.message}`, 'error');
        console.error('Model loading error:', error);
      }
    }

    async function loadTokenizer() {
      updateStatus('Loading tokenizer...', 'info', true);
      tokenizer = await Tokenizer.fromUrl(tokenizerUrl, { type: 'BPE' });
      updateStatus('‚úì Tokenizer loaded', 'success');
    }

    function createInitialKVCache() {
      const batch = 1;
      const num_kv_heads = 2;
      const head_dim = 64;
      const layers = 24;
      const past_seq_len = 0;

      const kvCache = {};
      for (let i = 0; i < layers; i++) {
        kvCache[`past_key_values.${i}.key`] =
          new ort.Tensor('float32', new Float32Array(batch * num_kv_heads * past_seq_len * head_dim),
            [batch, num_kv_heads, past_seq_len, head_dim]);
        kvCache[`past_key_values.${i}.value`] =
          new ort.Tensor('float32', new Float32Array(batch * num_kv_heads * past_seq_len * head_dim),
            [batch, num_kv_heads, past_seq_len, head_dim]);
      }
      return kvCache;
    }

    async function runInference(prompt, maxLength=50) {
      if (!session || !tokenizer) {
        updateStatus('‚ùå Model or tokenizer not loaded', 'error');
        return;
      }

      outputEl.classList.remove('show');
      updateStatus('Running inference...', 'info', true);

      // Tokenize
      const enc = await tokenizer.encode(prompt);
      let inputIds = enc.ids.map(id => BigInt(id));
      let attentionMask = inputIds.map(_ => 1n);
      let positionIds = inputIds.map((_, i) => BigInt(i));

      const batch = 1;

      let kvCache = createInitialKVCache();

      const generated = [...inputIds];

      for (let step = 0; step < maxLength; step++) {
        const inputIdsTensor = new ort.Tensor('int64',
          BigInt64Array.from(inputIds), [batch, inputIds.length]);

        const attentionMaskTensor = new ort.Tensor('int64',
          BigInt64Array.from(attentionMask), [batch, attentionMask.length]);

        const positionIdsTensor = new ort.Tensor('int64',
          BigInt64Array.from(positionIds), [batch, positionIds.length]);

        const feeds = {
          input_ids: inputIdsTensor,
          attention_mask: attentionMaskTensor,
          position_ids: positionIdsTensor,
          ...kvCache
        };

        const results = await session.run(feeds);

        const logits = results.logits.data;
        // Greedy decode: pick the highest logit of last token
        const vocabSize = logits.length / (inputIds.length * batch);
        const lastLogits = logits.slice((inputIds.length - 1) * vocabSize);
        let maxIdx = 0;
        let maxVal = lastLogits[0];
        for (let i = 1; i < lastLogits.length; i++) {
          if (lastLogits[i] > maxVal) { maxVal = lastLogits[i]; maxIdx = i; }
        }

        const nextToken = BigInt(maxIdx);
        generated.push(nextToken);

        // Update for next iteration
        inputIds = [nextToken];
        attentionMask = [1n];
        positionIds = [BigInt(positionIds[positionIds.length - 1] + 1n)];

        // Update kvCache with present_* tensors
        for (let i = 0; i < 24; i++) {
          kvCache[`past_key_values.${i}.key`] = results[`present.${i}.key`];
          kvCache[`past_key_values.${i}.value`] = results[`present.${i}.value`];
        }

        // Stop if EOS
        if (nextToken === BigInt(151645)) break; // eos_token_id

      } // end for steps

      const decoded = await tokenizer.decode(Array.from(generated, x => Number(x)));
      updateStatus('‚úì Inference completed', 'success');
      outputContent.textContent = decoded;
      outputEl.classList.add('show');
    }

    runBtn.addEventListener('click', () => {
      runBtn.disabled = true;
      runInference(inputText.value.trim(), 50)
        .catch(err => {
          updateStatus(`‚ùå Inference error: ${err.message}`, 'error');
          console.error('Inference error:', err);
        })
        .finally(() => { runBtn.disabled = false; });
    });

    (async () => {
      const ok = await checkWebGPU();
      if (ok) {
        await loadTokenizer();
        await loadModel();
      }
    })();
  </script>
</body>
</html>
